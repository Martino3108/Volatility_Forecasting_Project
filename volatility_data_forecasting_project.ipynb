{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.002195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0.001747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  time_id    target\n",
       "0         0        5  0.004136\n",
       "1         0       11  0.001445\n",
       "2         0       16  0.002168\n",
       "3         0       31  0.002195\n",
       "4         0       62  0.001747"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the train dataset with volatility targets for each stock and time id\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the whole book data\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "import glob\n",
    "list_order_book_file_train = glob.glob('book_train.parquet/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_return(list_stock_prices):\n",
    "    return np.log(list_stock_prices).diff() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def realized_volatility(series_log_return):\n",
    "    return np.sqrt(np.sum(series_log_return**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute realized volatility for each time id\n",
    "def realized_volatility_per_time_id(file_path, prediction_column_name):\n",
    "    df_book_data = pd.read_parquet(file_path)\n",
    "    df_book_data['wap'] =(df_book_data['bid_price1'] * df_book_data['ask_size1']+df_book_data['ask_price1'] * df_book_data['bid_size1'])  / (\n",
    "                                      df_book_data['bid_size1']+ df_book_data[\n",
    "                                  'ask_size1'])\n",
    "    df_book_data['log_return'] = df_book_data.groupby(['time_id'])['wap'].transform(log_return)\n",
    "    df_book_data = df_book_data[~df_book_data['log_return'].isnull()]\n",
    "    df_realized_vol_per_stock =  pd.DataFrame(df_book_data.groupby(['time_id'])['log_return'].agg(realized_volatility)).reset_index()\n",
    "    df_realized_vol_per_stock = df_realized_vol_per_stock.rename(columns = {'log_return':prediction_column_name})\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_realized_vol_per_stock['row_id'] = df_realized_vol_per_stock['time_id'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    return df_realized_vol_per_stock[['row_id',prediction_column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute realized volatility for 100 seconds intervals in each time id (600 seconds)\n",
    "def realized_volatility_per_time_id_every_100_seconds(file_path, prediction_column_prefix):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    df_book_data = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Calcolo WAP\n",
    "    df_book_data['wap'] = (\n",
    "        df_book_data['bid_price1'] * df_book_data['ask_size1'] +\n",
    "        df_book_data['ask_price1'] * df_book_data['bid_size1']\n",
    "    ) / (df_book_data['bid_size1'] + df_book_data['ask_size1'])\n",
    "    \n",
    "    # Creiamo gli intervalli temporali\n",
    "    df_book_data['interval'] = (df_book_data['seconds_in_bucket'] // 100) * 100  # 0, 100, ..., 500\n",
    "    df_book_data = df_book_data[df_book_data['interval'] <= 500]\n",
    "\n",
    "    results = []\n",
    "    stock_id = file_path.split('=')[1]\n",
    "\n",
    "    for (time_id, interval), group in df_book_data.groupby(['time_id', 'interval']):\n",
    "        group = group.sort_values('seconds_in_bucket')\n",
    "        group['log_return'] = log_return(group['wap'])\n",
    "        group = group.dropna(subset=['log_return'])\n",
    "        if not group.empty:\n",
    "            vol = realized_volatility(group['log_return'])\n",
    "            row_id = f'{stock_id}-{time_id}'\n",
    "            results.append({\n",
    "                'row_id': row_id,\n",
    "                'interval': interval,\n",
    "                'volatility': vol\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    # Pivot in modo sicuro: riga = row_id, colonne = intervalli, valori = volatilitÃ \n",
    "    df_pivot = df_results.pivot_table(index='row_id', columns='interval', values='volatility')\n",
    "\n",
    "    # Rinomina le colonne in modo chiaro\n",
    "    df_pivot.columns = [f\"{prediction_column_prefix}_{int(c)}\" for c in df_pivot.columns]\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "\n",
    "    return df_pivot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate for each stock\n",
    "def past_realized_volatility_per_stock(list_file,prediction_column_prefix):\n",
    "    df_past_realized = pd.DataFrame()\n",
    "    \n",
    "    for file in list_file:\n",
    "            df_one_stock = realized_volatility_per_time_id(file, prediction_column_prefix)\n",
    "            df_past_realized = pd.concat([df_past_realized, df_one_stock], axis=0, ignore_index=True)\n",
    "    return df_past_realized\n",
    "df_past_realized_train = past_realized_volatility_per_stock(list_file=list_order_book_file_train,\n",
    "                                                           prediction_column_prefix='sigma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate for each stock (100 seconds batches version)\n",
    "def past_realized_volatility_per_stock_every_100_seconds(list_file,prediction_column_prefix):\n",
    "    df_past_realized = pd.DataFrame()\n",
    "    \n",
    "    for file in list_file:\n",
    "            df_one_stock = realized_volatility_per_time_id_every_100_seconds(file, prediction_column_prefix)\n",
    "            df_past_realized = pd.concat([df_past_realized, df_one_stock], axis=0, ignore_index=True)\n",
    "    return df_past_realized\n",
    "\n",
    "#run 20 minutes\n",
    "df_past_realized_train_every_100_seconds = past_realized_volatility_per_stock_every_100_seconds(list_file=list_order_book_file_train,\n",
    "                                                           prediction_column_prefix='sigma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset that contains a row id (stock id-time id) and all the realized volatilities (100 seconds batches)\n",
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "train = train[['row_id', 'target']]\n",
    "\n",
    "volatility_columns = ['row_id'] + [col for col in df_past_realized_train_every_100_seconds.columns if col.startswith('sigma')]\n",
    "\n",
    "df_joined_every_100_seconds = train.merge(df_past_realized_train_every_100_seconds[volatility_columns], on='row_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset that contains a row id (stock id-time id) and the realized volatility\n",
    "train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "train = train[['row_id', 'target']]\n",
    "\n",
    "volatility_columns = ['row_id'] + [col for col in df_past_realized_train.columns if col.startswith('sigma')]\n",
    "\n",
    "df_joined = train.merge(df_past_realized_train[volatility_columns], on='row_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           row_id    target     sigma\n",
      "0             0-5  0.004136  0.004499\n",
      "1            0-11  0.001445  0.001204\n",
      "2            0-16  0.002168  0.002369\n",
      "3            0-31  0.002195  0.002574\n",
      "4            0-62  0.001747  0.001894\n",
      "...           ...       ...       ...\n",
      "428927  126-32751  0.003461  0.003691\n",
      "428928  126-32753  0.003113  0.004104\n",
      "428929  126-32758  0.004070  0.003118\n",
      "428930  126-32763  0.003357  0.003661\n",
      "428931  126-32767  0.002090  0.002091\n",
      "\n",
      "[428932 rows x 3 columns]\n",
      "           row_id    target   sigma_0  sigma_100  sigma_200  sigma_300  \\\n",
      "0             0-5  0.004136  0.001978   0.002181   0.001689   0.001816   \n",
      "1            0-11  0.001445  0.000387   0.000521   0.000218   0.000298   \n",
      "2            0-16  0.002168  0.000864   0.001049   0.001398   0.000535   \n",
      "3            0-31  0.002195  0.000336   0.001725   0.000535   0.001466   \n",
      "4            0-62  0.001747  0.000987   0.000383   0.000393   0.000498   \n",
      "...           ...       ...       ...        ...        ...        ...   \n",
      "428927  126-32751  0.003461  0.001195   0.000916   0.001717   0.002269   \n",
      "428928  126-32753  0.003113  0.000778   0.001445   0.001489   0.001933   \n",
      "428929  126-32758  0.004070  0.000429   0.001204   0.000296   0.001813   \n",
      "428930  126-32763  0.003357  0.001673   0.001323   0.001782   0.001547   \n",
      "428931  126-32767  0.002090  0.000724   0.001179   0.000677   0.000907   \n",
      "\n",
      "        sigma_400  sigma_500  \n",
      "0        0.001778   0.001459  \n",
      "1        0.000371   0.000857  \n",
      "2        0.000990   0.000603  \n",
      "3        0.000154   0.000942  \n",
      "4        0.000891   0.001115  \n",
      "...           ...        ...  \n",
      "428927   0.000862   0.001539  \n",
      "428928   0.001089   0.002532  \n",
      "428929   0.001628   0.001164  \n",
      "428930   0.001501   0.000997  \n",
      "428931   0.000856   0.000654  \n",
      "\n",
      "[428932 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#sigma_0 is the realized volatility between second 0 and 100, sigma_100 between 100 and 200, and so on\n",
    "print(df_joined)\n",
    "print(df_joined_every_100_seconds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
